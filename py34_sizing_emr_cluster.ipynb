{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import OrderedDict, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The contents of this notebook are a version of what appears in http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/\n",
    "\n",
    "While there are many scattered sources on the Internet for configuring an EMR cluster for use with Spark, I found the link above to be the most useful. Kudos to Anthony Shipman for writing the page (and providing the spreadsheet, Apache Spark Config Cheatsheet - xlsx, in the contents). Note that if you are trying to configure Spark in cluster mode on EMR then refer to the \"cluster\" tab in that spreadsheet. \n",
    "\n",
    "Why this notebook then? It's just that I find this \"notebook\" version to be easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resource = namedtuple('Resource', 'memory_per_node_GB cores_per_node')\n",
    "\n",
    "# The first number is yarn.scheduler.maximum-allocation-mb \n",
    "#    from https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html#emr-hadoop-task-config-r3\n",
    "# The second number is the cores per node that are available for Spark’s use. \n",
    "#    If using Yarn, this will be the number of cores per machine managed by Yarn Resource Manager.\n",
    "#    from https://aws.amazon.com/ec2/instance-types/\n",
    "basic_info = {'r5.2xlarge' : Resource(57.344, 8),\\\n",
    "              'r5.4xlarge' : Resource(122.880, 16),\\\n",
    "              'r5.8xlarge' : Resource(253.952, 32),\\\n",
    "              'r5.12xlarge' : Resource(385.024, 48),\\\n",
    "              'r3.8xlarge' : Resource(241.644, 64),\\\n",
    "              'r3.4xlarge' : Resource(116.736, 32),\\\n",
    "              'r3.2xlarge' : Resource(54.272, 16),\\\n",
    "              'm2.4xlarge' : Resource(61.440, 8), \\\n",
    "              'd2.2xlarge' : Resource(54.272, 16),\\\n",
    "              'c3.2xlarge' : Resource(11.520, 8),\\\n",
    "              'c3.xlarge' : Resource(5.632, 4),\\\n",
    "              'm3.xlarge' : Resource(11.520, 8),\\\n",
    "              'r4.4xlarge': Resource(116.736, 16),\\\n",
    "              'i3.2xlarge': Resource(0.85*61, 8)\n",
    "             }\n",
    "# FYI:\n",
    "# The Amazon console for a cluster with r3.8xlarges shows 64 vCPUs listed in the hardware but \n",
    "# https://aws.amazon.com/ec2/instance-types/ shows 32 vCPUs for this instance type(WTF!) \n",
    "# Now using 64 instead of 32 allowed me to max out the CPU usage (as verified via Ganglia).\n",
    "# Thus for vCPUs in the dictionary above I am using what I saw reported in the Amazon console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended parameter settings\n",
    "\n",
    "#The percentage of memory in each executor that will be reserved for spark.yarn.executor.memoryOverhead.\n",
    "memory_overhead_coefficient = 0.1 \n",
    "\n",
    "#The upper bound for executor memory. \n",
    "# Each executor runs on its own JVM. Upwards of 64GB of memory and garbage collection issues can cause slowness\n",
    "executor_memory_upper_bound_GB = 64\n",
    "\n",
    "#The upper bound for number of cores per executor. \n",
    "# More than 5 cores per executor can degrade HDFS I/O throughput. \n",
    "# I believe this value can safely be increased if writing to a web-based “file system” such as S3, but significant increases to this limit are not recommended.\n",
    "executor_core_upper_bound = 5\n",
    "\n",
    "# Cores per machine to reserve for OS processes. \n",
    "#Should be zero if only a percentage of the machine’s cores were made available to Spark \n",
    "#(i.e. entered in the cores_per_node input above).\n",
    "os_reserved_cores_per_node = 1\n",
    "\n",
    "#The amount of RAM per machine to reserve for OS processes. \n",
    "#Should be zero if only a percentage of the machine’s RAM was made available to Spark \n",
    "#(i.e. entered in the memory_per_node_GB input above).\n",
    "os_reserved_memory_per_node_GB = 1\n",
    "\n",
    "# The level of parallelism per allocated core. This field is used to determine the spark.default.parallelism setting. \n",
    "#Generally recommended setting for this value is double the number of cores.\n",
    "parallelism_per_core = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "\n",
    "instance_type_worker_node = 'r3.8xlarge'\n",
    "\n",
    "# The number of worker machines in your cluster. This can be as low as one machine.\n",
    "number_of_nodes = 8\n",
    "\n",
    "#input the value in yarn.scheduler.maximum-allocation-mb divided by 1000\n",
    "#http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html\n",
    "#  The amount of RAM per node that is available for Spark’s use. \n",
    "#  If using Yarn, this will be the amount of RAM per machine managed by Yarn Resource Manager.\n",
    "memory_per_node_GB = basic_info[instance_type_worker_node].memory_per_node_GB \n",
    "\n",
    "cores_per_node = basic_info[instance_type_worker_node].cores_per_node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available_memory_per_node_GB = 240.644,\n",
      "available_memory_per_node_GB_flr = 240,\n",
      "available_cores_per_node = 63\n"
     ]
    }
   ],
   "source": [
    "# calculated values\n",
    "available_memory_per_node_GB = memory_per_node_GB - os_reserved_memory_per_node_GB\n",
    "available_memory_per_node_GB_flr = math.floor(available_memory_per_node_GB)\n",
    "available_cores_per_node = cores_per_node - os_reserved_cores_per_node \n",
    "print('available_memory_per_node_GB = {0},\\navailable_memory_per_node_GB_flr = {1},\\navailable_cores_per_node = {2}'.format(available_memory_per_node_GB, available_memory_per_node_GB_flr, available_cores_per_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up_df = pd.DataFrame.from_dict( {'executors_per_node' : range(1,193) } )\n",
    "\n",
    "look_up_df['total_memory_per_executor_GB'] = look_up_df['executors_per_node']\\\n",
    "            .map(lambda z : math.floor(available_memory_per_node_GB_flr/z))\n",
    "\n",
    "look_up_df['overhead_memory_per_executor_GB'] = look_up_df['total_memory_per_executor_GB']\\\n",
    "            .map(lambda z : math.ceil( z * memory_overhead_coefficient))\n",
    "\n",
    "look_up_df['memory_per_executor_GB'] = look_up_df['total_memory_per_executor_GB'] - \\\n",
    "                                        look_up_df['overhead_memory_per_executor_GB']\n",
    "look_up_df['cores_per_executor'] = look_up_df['executors_per_node']\\\n",
    "            .map(lambda z : math.floor(available_cores_per_node/z) )\n",
    "\n",
    "look_up_df['unused_memory_per_node_GB'] = available_memory_per_node_GB - \\\n",
    "    (look_up_df['executors_per_node'] * look_up_df['total_memory_per_executor_GB'])\n",
    "\n",
    "look_up_df['unused_cores_per_node'] = available_cores_per_node - \\\n",
    "    (look_up_df['executors_per_node'] * look_up_df['cores_per_executor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - For 1 executors per node the Total Memory Per Executor exceeds the Executor Memory Upper Bound\n",
      "Warning - For 1 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Warning - For 2 executors per node the Total Memory Per Executor exceeds the Executor Memory Upper Bound\n",
      "Warning - For 2 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Warning - For 3 executors per node the Total Memory Per Executor exceeds the Executor Memory Upper Bound\n",
      "Warning - For 3 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Warning - For 4 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 4 executors per node the unused cores per node is 3\n",
      "Warning - For 5 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 5 executors per node the unused cores per node is 3\n",
      "Warning - For 6 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 6 executors per node the unused cores per node is 3\n",
      "Warning - For 7 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 7 executors per node the unused memory per node is 2.6440000000000055\n",
      "Warning - For 8 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 8 executors per node the unused cores per node is 7\n",
      "Warning - For 9 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 9 executors per node the unused memory per node is 6.6440000000000055\n",
      "Warning - For 10 executors per node the Cores Per Executor exceeds the Executor Core Upper Bound\n",
      "Note - For 10 executors per node the unused cores per node is 3\n",
      "Note - For 11 executors per node the unused memory per node is 9.644000000000005\n",
      "Note - For 11 executors per node the unused cores per node is 8\n",
      "**** We recommend using 11 executors per node ****\n",
      "****  This will result in 88 executors across the cluster\n",
      "****  This will result in 440 cores used across the cluster\n"
     ]
    }
   ],
   "source": [
    "selected_executors_per_node = None\n",
    "for candidate_executors_per_node in look_up_df['executors_per_node']:\n",
    "    myrow = look_up_df[look_up_df.executors_per_node == candidate_executors_per_node ]\n",
    "    \n",
    "    warnings = 0\n",
    "\n",
    "    if ( myrow['total_memory_per_executor_GB'].values[0] > executor_memory_upper_bound_GB ):\n",
    "        warnings += 1\n",
    "        print('Warning - For {0} executors per node the Total Memory Per Executor exceeds the Executor Memory Upper Bound'.format(candidate_executors_per_node) )\n",
    "    if ( myrow['cores_per_executor'].values[0] > executor_core_upper_bound ):\n",
    "        warnings += 1\n",
    "        print('Warning - For {0} executors per node the Cores Per Executor exceeds the Executor Core Upper Bound'.format(candidate_executors_per_node) )\n",
    "    if ( myrow['unused_memory_per_node_GB'].values[0] > 1 ):\n",
    "        print('Note - For {0} executors per node the unused memory per node is {1}'.format(candidate_executors_per_node, myrow['unused_memory_per_node_GB'].values[0]) )\n",
    "    if ( myrow['unused_cores_per_node'].values[0] > 1 ):\n",
    "        print('Note - For {0} executors per node the unused cores per node is {1}'.format(candidate_executors_per_node, myrow['unused_cores_per_node'].values[0]) )\n",
    "    if warnings == 0:\n",
    "        print('**** We recommend using {0} executors per node ****'.format(candidate_executors_per_node) )\n",
    "        print('****  This will result in {0} executors across the cluster'.format( (candidate_executors_per_node * number_of_nodes) ) )\n",
    "        print('****  This will result in {0} cores used across the cluster'.format( ( number_of_nodes * candidate_executors_per_node * myrow['cores_per_executor'].values[0]) ) )\n",
    "        selected_executors_per_node = candidate_executors_per_node\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note - For 11 executors per node the unused memory per node is 9.644000000000005 GB\n",
      "Note - For 11 executors per node the unused cores per node is 8\n",
      "****  This will result in 87 executors across the workers in the cluster\n",
      "****  This will result in 435 tasks running across the workers in the cluster at one time\n"
     ]
    }
   ],
   "source": [
    "# Select the number of executors per node you would like to have\n",
    "# A good rule of thumb for selecting the optimal number of Executors Per Node \n",
    "# would be to select the setting that minimizes Unused Memory Per Node \n",
    "# and Unused Cores Per Node while keeping Total Memory Per Executor \n",
    "# below the Executor Memory Upper Bound and Core Per Executor \n",
    "# below the Executor Core Upper Bound.\n",
    "\n",
    "# I will use the recommendation spit out by the cell before this one\n",
    "# you can provide a different number if you prefer\n",
    "spark_executor_cores_per_node = selected_executors_per_node \n",
    "\n",
    "myrow = look_up_df[look_up_df.executors_per_node == spark_executor_cores_per_node ]\n",
    "\n",
    "if ( myrow['total_memory_per_executor_GB'].values[0] > executor_memory_upper_bound_GB ):\n",
    "    print('Warning - Total Memory Per Executor exceeds the Executor Memory Upper Bound' )\n",
    "if ( myrow['cores_per_executor'].values[0] > executor_core_upper_bound ):\n",
    "    print('Warning - Cores Per Executor exceeds the Executor Core Upper Bound' )\n",
    "if ( myrow['unused_memory_per_node_GB'].values[0] > 1 ):\n",
    "    print('Note - For {0} executors per node the unused memory per node is {1} GB'.format(selected_executors_per_node, myrow['unused_memory_per_node_GB'].values[0]) )\n",
    "if ( myrow['unused_cores_per_node'].values[0] > 1 ):\n",
    "    print('Note - For {0} executors per node the unused cores per node is {1}'.format(selected_executors_per_node, myrow['unused_cores_per_node'].values[0]) ) \n",
    "print('****  This will result in {0} executors across the workers in the cluster'.format( ( (selected_executors_per_node * number_of_nodes) - 1 ) ) )    \n",
    "print('****  This will result in {0} tasks running across the workers in the cluster at one time'.format( ( (number_of_nodes * selected_executors_per_node - 1 ) * myrow['cores_per_executor'].values[0] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('spark.executor.instances', 87),\n",
       "             ('spark.yarn.executor.memoryOverhead', 3072),\n",
       "             ('spark.executor.memory', 18),\n",
       "             ('spark.yarn.driver.memoryOverhead', 3072),\n",
       "             ('spark.driver.memory', 18),\n",
       "             ('spark.executor.cores', 5),\n",
       "             ('spark.driver.cores', 5),\n",
       "             ('spark.default.parallelism', 1914)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark configs\n",
    "\n",
    "# this should be equal (or close enough) to the number of executors shown in the 'Executors' tab of the Spark Console\n",
    "# look at section below the summary \n",
    "# This is the number of total executors in your cluster. \n",
    "#We subtract one to account for the driver. The driver will consume as many resources as we are allocating to an individual executor on one, and only one, of our nodes.\n",
    "spark_executor_instances = ((number_of_nodes * selected_executors_per_node) -1)\n",
    "\n",
    "spark_yarn_executor_memory_overhead =  1024 * myrow['overhead_memory_per_executor_GB'].values[0]\n",
    "\n",
    "# Assuming tasks are available, Spark will run one task per core that is available to the executor\n",
    "# verify this by looking at 'Executors' tab of the Spark Console (look below Summary section at Cores/ActiveTasks)\n",
    "spark_cores_per_executor = myrow['cores_per_executor'].values[0]\n",
    "\n",
    "#default number of partitions for Spark RDDs, Dataframes, etc.\n",
    "#('spark.scheduler.mode', 'FAIR')\n",
    "spark_default_parallelism = spark_executor_instances * spark_executor_cores_per_node * parallelism_per_core\n",
    "\n",
    "spark_configs = OrderedDict([ ('spark.executor.instances', spark_executor_instances), \\\n",
    "    ('spark.yarn.executor.memoryOverhead',spark_yarn_executor_memory_overhead),\\\n",
    "    ('spark.executor.memory',myrow['memory_per_executor_GB'].values[0]), \\\n",
    "    ('spark.yarn.driver.memoryOverhead', spark_yarn_executor_memory_overhead), \\\n",
    "    ('spark.driver.memory', myrow['memory_per_executor_GB'].values[0]), \\\n",
    "    ('spark.executor.cores',spark_cores_per_executor), \\\n",
    "    ('spark.driver.cores',spark_cores_per_executor), \\\n",
    "    ('spark.default.parallelism', spark_default_parallelism )\n",
    "])\n",
    "spark_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>executors_per_node</th>\n",
       "      <th>total_memory_per_executor_GB</th>\n",
       "      <th>overhead_memory_per_executor_GB</th>\n",
       "      <th>memory_per_executor_GB</th>\n",
       "      <th>cores_per_executor</th>\n",
       "      <th>unused_memory_per_node_GB</th>\n",
       "      <th>unused_cores_per_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>9.644</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    executors_per_node  total_memory_per_executor_GB  \\\n",
       "10                  11                            21   \n",
       "\n",
       "    overhead_memory_per_executor_GB  memory_per_executor_GB  \\\n",
       "10                                3                      18   \n",
       "\n",
       "    cores_per_executor  unused_memory_per_node_GB  unused_cores_per_node  \n",
       "10                   5                      9.644                      8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_up_df[look_up_df.executors_per_node == spark_executor_cores_per_node ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
